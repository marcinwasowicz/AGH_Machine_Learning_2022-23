{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5410db1b",
   "metadata": {},
   "source": [
    "# Introductory Example of Ensemble Learning in Knowledge Distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f552fa67",
   "metadata": {},
   "source": [
    "### Define Single Teacher Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c0a1032",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "TRAIN_TEACHERS = False\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linear1 = nn.Linear(784, 128)\n",
    "        self.linear2 = nn.Linear(128, 128)\n",
    "        self.linear3 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, data):\n",
    "        data = data.view(data.size(0), -1)  # flatten\n",
    "        output = F.relu(self.linear1(data))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        output = self.linear3(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b84a558",
   "metadata": {},
   "source": [
    "### Define Single Teacher and Ensemble of Teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3e35d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchensemble import BaggingClassifier, FusionClassifier\n",
    "\n",
    "teacher_ensemble = BaggingClassifier(\n",
    "    estimator=MLP,\n",
    "    n_estimators=10,\n",
    "    cuda=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce39a6ef",
   "metadata": {},
   "source": [
    "### Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d95438b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    ")\n",
    "\n",
    "train = datasets.MNIST(\"../Dataset\", train=True, download=True, transform=transform)\n",
    "test = datasets.MNIST(\"../Dataset\", train=False, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=128, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574c32dd",
   "metadata": {},
   "source": [
    "### Offline Teacher Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f2696b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_TEACHERS:\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = {\"optimizer_name\": \"Adam\", \"lr\": 1e-3, \"weight_decay\": 5e-4}\n",
    "\n",
    "    teacher_ensemble.set_criterion(criterion)\n",
    "    teacher_ensemble.set_optimizer(**optimizer)\n",
    "\n",
    "    print(\"Training Teacher Ensemble\")\n",
    "    teacher_ensemble.fit(train_loader, epochs=20, test_loader=test_loader)\n",
    "else:\n",
    "    from torchensemble.utils import io\n",
    "    io.load(teacher_ensemble, \".\")\n",
    "    \n",
    "print(str(teacher_ensemble.evaluate(test_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ee048f",
   "metadata": {},
   "source": [
    "### Define Student Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5b3c1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StudentMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StudentMLP, self).__init__()\n",
    "        self.linear1 = nn.Linear(784, 16)\n",
    "        self.linear2 = nn.Linear(16, 10)\n",
    "\n",
    "    def forward(self, data):\n",
    "        data = data.view(data.size(0), -1)  # flatten\n",
    "        output = F.relu(self.linear1(data))\n",
    "        output = self.linear2(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53149bb0",
   "metadata": {},
   "source": [
    "### Implement Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea9705a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "\n",
    "def train_loop(student, train_loader, test_loader, epochs):\n",
    "    student.train()\n",
    "    optimizer = Adam(student.parameters(), lr=1e-3)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_epoch_loss = 0.0\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            student_outputs = student(inputs)\n",
    "            loss = F.cross_entropy(student_outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_epoch_loss += loss.item()\n",
    "\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data in test_loader:\n",
    "                inputs, labels = data\n",
    "                outputs = student(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(\n",
    "            f\"Epoch: {epoch}, Accuracy: {100 * correct/total}, Loss: {total_epoch_loss}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8deb055e",
   "metadata": {},
   "source": [
    "### Implement KD Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d91fcb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kd_train_loop(student, teacher, alpha, temperature, train_loader, test_loader, epochs):\n",
    "    student.train()\n",
    "    teacher.eval()\n",
    "\n",
    "    optimizer = Adam(student.parameters(), lr=1e-3)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_epoch_loss = 0.0\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            student_outputs = student(inputs)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                teacher_outputs = teacher(inputs)\n",
    "\n",
    "            loss = nn.KLDivLoss()(\n",
    "                F.log_softmax(student_outputs/temperature, dim=1), F.softmax(teacher_outputs/temperature, dim=1)\n",
    "            ) * (alpha) * temperature**2 + F.cross_entropy(student_outputs, labels) * (1.0 - alpha)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_epoch_loss += loss.item()\n",
    "\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data in test_loader:\n",
    "                inputs, labels = data\n",
    "                outputs = student(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(\n",
    "            f\"Epoch: {epoch}, Accuracy: {100 * correct/total}, Loss: {total_epoch_loss}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86020f13",
   "metadata": {},
   "source": [
    "### Train Student Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d837d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_1 = StudentMLP()\n",
    "student_2 = StudentMLP()\n",
    "\n",
    "print(\"No teacher training\")\n",
    "train_loop(student_1, train_loader, test_loader, 10)\n",
    "\n",
    "print(\"Teacher Ensemble\")\n",
    "kd_train_loop(student_2, teacher_ensemble, 0.95, 10, train_loader, test_loader, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('agh_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "41b19489afcf2c7312096d4bb8c3c92e9eaa3192e5e7819118633a90bd50e0cc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
